{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "beginning-matter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]/home/ddatta/anaconda3/envs/text/lib/python3.7/site-packages/ipykernel_launcher.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :: 0.7536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-f5de9dc77177>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'BERT'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m \u001b[0mreport1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./results_{}.txt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-f5de9dc77177>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(model_type, num_epochs, log_interval)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mb_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss :: {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mb_idx\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/text/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/text/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas \n",
    "import os\n",
    "import sys\n",
    "import json \n",
    "from spacy.tokenizer import Tokenizer\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizerFast, ReformerTokenizer,ReformerTokenizerFast\n",
    "from transformers import DistilBertForSequenceClassification, BertForSequenceClassification, ReformerForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AlbertTokenizer, AlbertModel\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "NLP_object = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def obtain_title_cleaned_v1(text):\n",
    "    global NLP_object\n",
    "    doc = NLP_object(text)\n",
    "    punct_removed_title = [ _.lemma_.lower() for _ in doc if not _.is_punct]\n",
    "    return punct_removed_title\n",
    "\n",
    "labelled_data_folders = {\n",
    "    1 : './../../Data_042021/ground_truth/positive/',\n",
    "    0 : './../../Data_042021/ground_truth/negative/',\n",
    "}\n",
    "\n",
    "def get_titles(file):  \n",
    "    object_dict = {}\n",
    "    with open(file, 'r') as fh:\n",
    "        for _line_ in fh.readlines():\n",
    "            _obj_ = json.loads(_line_)\n",
    "            _id_ = _obj_['id'] \n",
    "            object_dict[_id_] = _obj_ \n",
    "            \n",
    "    text_list = Parallel(n_jobs=mp.cpu_count(), prefer=\"threads\")(delayed(obtain_title_cleaned_v1)(obj['title'], ) for obj in object_dict.values())\n",
    "    return text_list\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Return titles\n",
    "# ===========================\n",
    "def get_labelled_titles():\n",
    "    labelled_title_dict = {}\n",
    "    for label, folder in labelled_data_folders.items():\n",
    "        files = glob.glob(os.path.join(folder, '**.json'))\n",
    "        title_list = []\n",
    "        for file in files:\n",
    "            _results = get_titles(file)\n",
    "            title_list.extend(_results)\n",
    "        labelled_title_dict[label] = title_list\n",
    "    return labelled_title_dict\n",
    "\n",
    "def get_tokenizer(model_type = 'BERT'):\n",
    "    if model_type =='distilBERT':\n",
    "        tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "    elif model_type =='BERT':\n",
    "        tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    elif model_type =='alBERT':\n",
    "        tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "    else:\n",
    "        print('model_type not allowed ', model_type)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_labelled_data():\n",
    "    labelled_title_dict = get_labelled_titles()\n",
    "    X = []\n",
    "    Y = []\n",
    "    for _label, _x in labelled_title_dict.items():\n",
    "        \n",
    "        if _label == 1 :\n",
    "            labels = np.ones(len(_x), dtype=int).tolist()\n",
    "        if _label == 0 :\n",
    "            labels = np.zeros(len(_x),  dtype=int).tolist()\n",
    "        X.extend(_x)\n",
    "        Y.extend(labels)\n",
    "    return X,Y\n",
    "        \n",
    "\n",
    "def get_train_test_data():\n",
    "    text, labels = get_labelled_data()  \n",
    "    train_text, test_text, train_label, test_label = train_test_split(text, labels, test_size=0.2)\n",
    "    return train_text, test_text, train_label, test_label\n",
    "\n",
    "class titleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def create_datasets(model_type='BERT'):\n",
    "    tokenizer = get_tokenizer(model_type)\n",
    "    train_text, test_text, train_label, test_label = get_train_test_data()\n",
    "    train_encodings = tokenizer(train_text, is_split_into_words=True, padding=True, truncation=True,  return_tensors='pt')\n",
    "    test_encodings = tokenizer(test_text, is_split_into_words=True, padding=True, truncation=True,  return_tensors='pt')\n",
    "    train_dataset = titleDataset(train_encodings, train_label)\n",
    "    test_dataset = titleDataset(test_encodings, test_label)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def run_model(model_type, num_epochs = 100, log_interval=75):\n",
    "    train_dataset, test_dataset = create_datasets(model_type)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "    if model_type=='BERT':\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "    elif model_type=='distilBERT':\n",
    "        model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "    elif model_type=='alBERT':\n",
    "        model = AlbertForSequenceClassification.from_pretrained('albert-base-v2')\n",
    "    model.train()\n",
    "    optim = AdamW(model.parameters(), lr=5e-5)\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        b_idx = 0\n",
    "        for batch in train_loader:\n",
    "            optim.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            if b_idx % log_interval == 0:\n",
    "                print('Loss :: {:.4f}'.format(np.mean(loss.cpu().data.numpy())))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            b_idx+=1\n",
    "            \n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predicted_label = torch.argmax(F.softmax(outputs.logits, dim=-1),dim=-1, keepdims=False)\n",
    "        _pred_y = predicted_label.cpu().data.numpy().tolist()\n",
    "        _true_y = labels.cpu().data.numpy().tolist()\n",
    "        y_pred.extend(_pred_y)\n",
    "        y_true.extend(_true_y)\n",
    "    print('MODEL {}'.format(model_type))\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    return report\n",
    "\n",
    "\n",
    "\n",
    "model_type = 'BERT'\n",
    "report1 = run_model(model_type=model_type, num_epochs = 2)\n",
    "with open('./results_{}.txt'.format(model_type),'w+') as fh:\n",
    "    fh.write(report1)\n",
    "\n",
    "model_type = 'alBERT'\n",
    "report2 = run_model(model_type=model_type, num_epochs = 100)\n",
    "with open('./results_{}.txt'.format(model_type),'w+') as fh:\n",
    "    fh.write(report2)\n",
    "\n",
    "\n",
    "model_type = 'distilBERT'\n",
    "report3 = run_model(model_type=model_type, num_epochs = 100)\n",
    "with open('./results_{}.txt'.format(model_type),'w+') as fh:\n",
    "    fh.write(report3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-clothing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
